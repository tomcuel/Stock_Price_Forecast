{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. Librairies\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Generate complex synthetic time series\n",
    "# ===============================\n",
    "def create_complex_time_series(n_samples=2000, seq_len=50, n_features=3, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    t = np.arange(n_samples + seq_len)\n",
    "    \n",
    "    series = []\n",
    "    for f in range(n_features):\n",
    "        # Different frequency sine/cosine\n",
    "        freq = np.random.uniform(0.01, 0.1)\n",
    "        amp = np.random.uniform(0.5, 2.0)\n",
    "        wave = amp * np.sin(2 * np.pi * freq * t) + amp/2 * np.cos(2 * np.pi * freq*0.5 * t)\n",
    "        \n",
    "        # Add trend\n",
    "        trend = t * np.random.uniform(0.0005, 0.002)\n",
    "        \n",
    "        # Add seasonal effect\n",
    "        season = 0.5 * np.sin(2 * np.pi * t / np.random.randint(30, 100))\n",
    "        \n",
    "        # Add noise\n",
    "        noise = np.random.normal(0, 0.2, len(t))\n",
    "        \n",
    "        # Add occasional spikes\n",
    "        spikes = np.zeros(len(t))\n",
    "        spike_idx = np.random.choice(len(t), size=int(0.01*len(t)), replace=False)\n",
    "        spikes[spike_idx] = np.random.uniform(1, 3, len(spike_idx))\n",
    "        \n",
    "        series.append(wave + trend + season + noise + spikes)\n",
    "    \n",
    "    series = np.stack(series, axis=-1)\n",
    "    \n",
    "    # Build sequences\n",
    "    X, y = [], []\n",
    "    for i in range(n_samples):\n",
    "        X.append(series[i:i+seq_len])\n",
    "        y.append(series[i+seq_len, 0])  # predict first feature as example\n",
    "    \n",
    "    X = np.array(X)  # (n_samples, seq_len, n_features)\n",
    "    y = np.array(y)  # (n_samples,)\n",
    "    return X, y, series\n",
    "\n",
    "\n",
    "SEQ_LEN = 50\n",
    "N_FEATURES = 3\n",
    "X, y, true_series = create_complex_time_series(n_samples=5000, seq_len=SEQ_LEN, n_features=N_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc4940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 2. Plotting the time series\n",
    "# ===============================\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(true_series[:, 0], label=\"True underlying target series\")\n",
    "plt.title(\"True Continuous Time Series\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b97aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================== \n",
    "# 3. Prepare Encoder–Decoder Data \n",
    "# # =============================== \n",
    "def build_encoder_decoder_dataset(X, y, dec_len=None): \n",
    "    \"\"\" Builds a dataset for seq2seq time series: \n",
    "    - X_enc: encoder input (past window) \n",
    "    - X_dec: decoder input (teacher-forcing, shifted window) \n",
    "    - Y: target values (next-step prediction) \n",
    "    \"\"\" \n",
    "    N, seq_len, n_features = X.shape \n",
    "    if dec_len is None: dec_len = seq_len \n",
    "    # Encoder input = original window \n",
    "    X_enc = X.copy() # (N, seq_len, n_features) \n",
    "    # Decoder input = 1-step shifted window \n",
    "    X_dec = np.zeros_like(X) # initialize \n",
    "    # Shift encoder window right by 1 timestep \n",
    "    X_dec[:, 1:, :] = X[:, :-1, :] # teacher forcing \n",
    "    X_dec[:, 0, :] = 0 # start token \n",
    "    # Target values (next value of feature 0) \n",
    "    Y = y.reshape(-1, 1) # (N, 1) \n",
    "    return X_enc, X_dec, Y \n",
    "\n",
    "# Build encoder–decoder dataset PRED_HORIZON = 20 \n",
    "X_enc, X_dec, Y = build_encoder_decoder_dataset(X, y, dec_len=SEQ_LEN) \n",
    "# Train/validation split \n",
    "train_size = int(0.8 * len(X)) \n",
    "X_enc_train, X_enc_val = X_enc[:train_size], X_enc[train_size:] \n",
    "X_dec_train, X_dec_val = X_dec[:train_size], X_dec[train_size:] \n",
    "Y_train, Y_val = Y[:train_size], Y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4. Attention Variants\n",
    "# ===============================\n",
    "class MultiQueryAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi-Query Attention: shared key/value projections across heads\n",
    "    Supports a query, key, value interface like standard MultiHeadAttention \n",
    "    (compatible drop-in replacement for MultiHeadAttention)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.supports_masking = True\n",
    "\n",
    "        # Query gets separate projection for each head\n",
    "        self.q_proj = layers.Dense(num_heads * key_dim)\n",
    "        # Keys and Values shared across all heads\n",
    "        self.k_proj = layers.Dense(key_dim)\n",
    "        self.v_proj = layers.Dense(key_dim)\n",
    "        # Final linear projection\n",
    "        self.out = layers.Dense(num_heads * key_dim)\n",
    "\n",
    "    def call(self, query, value=None, key=None, mask=None):\n",
    "        \"\"\"\n",
    "        query: (B, Tq, d_model)\n",
    "        key  : (B, Tk, d_model) or None\n",
    "        value: (B, Tk, d_model) or None\n",
    "        mask : (B, 1, Tq, Tk) optional\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "\n",
    "        B = tf.shape(query)[0]\n",
    "        Tq = tf.shape(query)[1]\n",
    "        Tk = tf.shape(key)[1]\n",
    "        \n",
    "        # Linear projection (Project Q, K, V)\n",
    "        q = self.q_proj(query)                  # (B, Tq, H*D)\n",
    "        k = self.k_proj(key)                    # (B, Tk, D)\n",
    "        v = self.v_proj(value)                  # (B, Tk, D)\n",
    "\n",
    "        # Reshape Q into heads\n",
    "        q = tf.reshape(q, (B, Tq, self.num_heads, self.key_dim))\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])        # (B, H, Tq, D)\n",
    "\n",
    "        # Expand shared Keys and Values across heads\n",
    "        k = k[:, None, :, :]                    # (B, 1, Tk, D)\n",
    "        v = v[:, None, :, :]                    # (B, 1, Tk, D)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = tf.matmul(q, k, transpose_b=True)  # (B, H, Tq, Tk)\n",
    "        scores /= tf.sqrt(tf.cast(self.key_dim, tf.float32))\n",
    "        # Apply attention mask\n",
    "        if mask is not None:\n",
    "            scores += mask  # broadcasting works if mask is (B, 1, Tq, Tk)\n",
    "\n",
    "        # Softmax over key dimension\n",
    "        attn = tf.nn.softmax(scores, axis=-1)\n",
    "        # Weighted sum\n",
    "        out = tf.matmul(attn, v)                # (B, H, Tq, D)\n",
    "\n",
    "        # Restore shape\n",
    "        out = tf.transpose(out, [0, 2, 1, 3])    # (B, Tq, H, D)\n",
    "        out = tf.reshape(out, (B, Tq, self.num_heads * self.key_dim))\n",
    "\n",
    "        # Final linear projection\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5.Transformer Encoder Block\n",
    "# ===============================\n",
    "class EncoderBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    One Transformer encoder block \n",
    "\n",
    "    Consists of:\n",
    "    1. Multi-Head (or Multi-Query) Self-Attention\n",
    "    2. Add & LayerNorm\n",
    "    3. Feed-Forward Network (FFN)\n",
    "    4. Add & LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, ff_layers=None, use_mqa=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Standard MHA or Multi-Query Attention\n",
    "        if use_mqa:\n",
    "            self.attn = MultiQueryAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        else:\n",
    "            self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "\n",
    "        # Feed-forward network\n",
    "        if ff_layers is None:\n",
    "            ff_layers = [128]\n",
    "        ffn_list = []\n",
    "        for ff_layer in ff_layers:\n",
    "            ffn_list.append(layers.Dense(ff_layer, activation='relu'))\n",
    "            ffn_list.append(layers.Dropout(0.1))\n",
    "        ffn_list.append(layers.Dense(d_model))\n",
    "        self.ffn = tf.keras.Sequential(ffn_list)\n",
    "\n",
    "        # Two LayerNorms for stabilizing training\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, d_model)\n",
    "        mask: (batch, 1, seq_len, seq_len) or None\n",
    "        \"\"\"\n",
    "        # 1. SELF-ATTENTION\n",
    "        if isinstance(self.attn, MultiQueryAttention):\n",
    "            attn_out = self.attn(query=x, value=x, key=x, mask=mask)\n",
    "        else:\n",
    "            attn_out = self.attn(query=x, value=x, key=x, attention_mask=mask)\n",
    "        # Residual connection + normalization\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        # 2. FEED FORWARD\n",
    "        ffn_output = self.ffn(x)\n",
    "        # Residual + normalization\n",
    "        return self.norm2(x + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 6. Decoder Block (masked + cross-attention)\n",
    "# ===============================\n",
    "class DecoderBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    One Transformer decoder block\n",
    "\n",
    "    Consists of:\n",
    "    1. Causal masked self-attention\n",
    "    2. Encoder-decoder cross-attention\n",
    "    3. Feed-forward network\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, ff_layers=None, use_mqa=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention (causal mask applied) and Cross-attention (decoder queries, encoder keys)\n",
    "        if use_mqa:\n",
    "            self.self_attn = MultiQueryAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.cross_attn = MultiQueryAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        else:\n",
    "            self.self_attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.cross_attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "\n",
    "        # Feed-forward network\n",
    "        if ff_layers is None:\n",
    "            ff_layers = [128]\n",
    "        ffn_list = []\n",
    "        for ff_layer in ff_layers:\n",
    "            ffn_list.append(layers.Dense(ff_layer, activation='relu'))\n",
    "            ffn_list.append(layers.Dropout(0.1))\n",
    "        ffn_list.append(layers.Dense(d_model))\n",
    "        self.ffn = tf.keras.Sequential(ffn_list)\n",
    "\n",
    "        # LayerNorms\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, x, enc_out, causal_mask=None, training=False):\n",
    "        \"\"\"\n",
    "        x       : decoder input (batch, dec_len, d_model)\n",
    "        enc_out : encoder output  (batch, enc_len, d_model)\n",
    "        causal_mask: prevents decoder from looking into the future\n",
    "        \"\"\"\n",
    "        # Masked self-attention and Cross-attention\n",
    "        if isinstance(self.self_attn, MultiQueryAttention):\n",
    "            attn1 = self.self_attn(query=x, value=x, key=x, mask=causal_mask)\n",
    "            attn2 = self.cross_attn(x, enc_out)\n",
    "        else:\n",
    "            attn1 = self.self_attn(query=x, value=x, key=x, attention_mask=causal_mask)\n",
    "            attn2 = self.cross_attn(query=x, value=enc_out, key=enc_out)\n",
    "        x = self.norm1(x + attn1)\n",
    "        x = self.norm2(x + attn2)\n",
    "\n",
    "        # Feed-forward\n",
    "        ffn_out = self.ffn(x)\n",
    "\n",
    "        # Final normalization\n",
    "        x = self.norm3(x + ffn_out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b17543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 7. Positional Encoding\n",
    "# ===============================\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    pos = np.arange(seq_len)[:, None]\n",
    "    i = np.arange(d_model)[None, :]\n",
    "    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pe = np.zeros_like(angle_rads)\n",
    "    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.cast(pe[None, ...], tf.float32)   # shape (1, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 8. Full Encoder–Decoder Transformer\n",
    "# ===============================\n",
    "class TransformerTimeSeries(tf.keras.Model):\n",
    "    def __init__(self, enc_len, dec_len, d_model=64, num_heads=4, \n",
    "                 enc_ff_layers=None, num_enc_layers=2, \n",
    "                 dec_ff_layers=None, num_dec_layers=2,\n",
    "                 use_mqa=False):\n",
    "        super().__init__()\n",
    "\n",
    "        #  Embeddings\n",
    "        self.enc_embedding = layers.Dense(d_model)\n",
    "        self.dec_embedding = layers.Dense(d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.enc_pos = positional_encoding(enc_len, d_model)\n",
    "        self.dec_pos = positional_encoding(dec_len, d_model)\n",
    "\n",
    "        # Encoder and decoder stacks\n",
    "        self.encoder = [EncoderBlock(d_model, num_heads, enc_ff_layers, use_mqa) for _ in range(num_enc_layers)]\n",
    "        self.decoder = [DecoderBlock(d_model, num_heads, dec_ff_layers, use_mqa) for _ in range(num_dec_layers)]\n",
    "        \n",
    "        self.out = layers.Dense(1)  # Final projection on the last step\n",
    "\n",
    "    def causal_mask(self, seq_len):\n",
    "        \"\"\"\n",
    "        Causal mask for decoder\n",
    "\n",
    "        Creates a triangular causal mask so that\n",
    "        position i can only attend to <= i\n",
    "        \"\"\"\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)   # Shape (1, 1, seq_len, seq_len)\n",
    "        mask = mask * -1e9\n",
    "        return mask[None, None, :, :]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        encoder_input, decoder_input = inputs\n",
    "\n",
    "        # ENCODER\n",
    "        x = self.enc_embedding(encoder_input) + self.enc_pos\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, training=training)\n",
    "        enc_out = x   # (B, enc_len, d_model)\n",
    "\n",
    "        # DECODER\n",
    "        y = self.dec_embedding(decoder_input) + self.dec_pos\n",
    "        # Create causal mask\n",
    "        mask = self.causal_mask(tf.shape(decoder_input)[1])\n",
    "        for layer in self.decoder:\n",
    "            y = layer(y, enc_out, causal_mask=mask, training=training)\n",
    "\n",
    "        # Output prediction\n",
    "        return self.out(y)[:, -1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 9. Create the model\n",
    "# ===============================\n",
    "model = TransformerTimeSeries(\n",
    "    enc_len=SEQ_LEN, dec_len=SEQ_LEN,\n",
    "    d_model=128, num_heads=8,\n",
    "\n",
    "    # FFN depths for encoder and decoder\n",
    "    enc_ff_layers=[512], num_enc_layers=3,\n",
    "    dec_ff_layers=[512], num_dec_layers=3,\n",
    "\n",
    "    use_mqa=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 10. Extra Metrics (give more meaningful evaluation)\n",
    "# ===============================\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean((y_true - y_pred) ** 2))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs((y_true - y_pred) / (y_true + 1e-6))) * 100\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 11. Learning Rate Scheduler: Cosine Decay + Warmup\n",
    "# ===============================\n",
    "steps_per_epoch = len(X_enc_train) // 32\n",
    "total_steps = steps_per_epoch * 20\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=1e-3, first_decay_steps=steps_per_epoch * 5, t_mul=1.5, m_mul=0.8, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 12. Optimizer: AdamW + Learning Schedule + Weight Decay\n",
    "# ===============================\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f49c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 13. Compile model\n",
    "# ===============================\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.Huber(delta=1.0),\n",
    "    metrics=[rmse, MeanAbsoluteError(), mape, r2_score]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29334187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 14. Callbacks\n",
    "# ===============================\n",
    "callbacks = [EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)]  # stop when validation stops improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 15. Training\n",
    "# ===============================\n",
    "history = model.fit(\n",
    "    (X_enc_train, X_dec_train), Y_train,\n",
    "    validation_data=((X_enc_val, X_dec_val), Y_val),\n",
    "    epochs=10,                # let early stopping stop earlier\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 16. Plot Predictions\n",
    "# ===============================\n",
    "X_train = (X_enc_train, X_dec_train)\n",
    "X_val = (X_enc_val, X_dec_val)\n",
    "\n",
    "def plot_predictions(model, X, y_true, n=500):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = y_pred[:n, 0, 0]  # reshape (B,1,1) → (B,)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(y_true[:n], label='True')\n",
    "    plt.plot(y_pred, label='Pred')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(model, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 17. Recursive Multi-step Forecasting\n",
    "# ===============================\n",
    "def recursive_forecast(model, X_enc_init, X_dec_init=None, n_future=100):\n",
    "    \"\"\"\n",
    "    Predict n_future steps recursively from initial sequence\n",
    "    \n",
    "    X_enc_init: (seq_len, n_features)  -> encoder input\n",
    "    X_dec_init: (seq_len, n_features)  -> decoder input, optional\n",
    "    \"\"\"\n",
    "    seq_enc = X_enc_init.copy()\n",
    "    if X_dec_init is None:\n",
    "        # Start decoder input with zeros + start token\n",
    "        seq_dec = np.zeros_like(seq_enc)\n",
    "        seq_dec[0, :] = 0\n",
    "        seq_dec[1:, :] = seq_enc[:-1, :]\n",
    "    else:\n",
    "        seq_dec = X_dec_init.copy()\n",
    "\n",
    "    predictions = []\n",
    "    for _ in range(n_future):\n",
    "        # Model expects a tuple: (encoder_input, decoder_input)\n",
    "        y_pred = model.predict((seq_enc[np.newaxis, :, :], seq_dec[np.newaxis, :, :]))  # shape (1,1,1)\n",
    "        pred = y_pred[0, 0, 0]\n",
    "        predictions.append(pred)\n",
    "        # Prepare next decoder input\n",
    "        new_step = np.zeros(seq_dec.shape[1])\n",
    "        new_step[0] = pred  # first feature predicted\n",
    "        seq_dec = np.vstack([seq_dec[1:], new_step])\n",
    "        # Optionally append prediction to encoder input for rolling predictions\n",
    "        seq_enc = np.vstack([seq_enc[1:], new_step])\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Choose the last validation sequence as starting point\n",
    "X_start_enc = X_enc_val[-1]\n",
    "X_start_dec = X_dec_val[-1]\n",
    "# Predict 20 future steps\n",
    "n_future = 20\n",
    "future_transformer = recursive_forecast(model, X_start_enc, X_start_dec, n_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 18. Plot Future Forecasts\n",
    "# ===============================\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(np.arange(len(Y_val[-200:])), Y_val[-200:], label='Recent True Values', color='blue')\n",
    "plt.plot(np.arange(len(Y_val[-200:]), len(Y_val[-200:]) + n_future), future_transformer, label='Transformer Forecast', color='green')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Multi-step Future Forecasting')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 19. Plot Predictions + Future Forecasts\n",
    "# ===============================\n",
    "def plot_predictions_and_forecast(model, X_val, y_val, n_past=200, n_future=100, label=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot past predictions on validation set and forecast n_future steps ahead\n",
    "    \n",
    "    X_val: tuple (X_enc_val, X_dec_val)\n",
    "    y_val: true values\n",
    "    \"\"\"\n",
    "    X_enc_val, X_dec_val = X_val\n",
    "    # Past predictions\n",
    "    y_pred_past = model.predict((X_enc_val[-n_past:], X_dec_val[-n_past:]))[:, 0, 0]\n",
    "    # Future forecast starting from last validation sequence\n",
    "    X_start_enc = X_enc_val[-1]\n",
    "    X_start_dec = X_dec_val[-1]\n",
    "    future_pred = recursive_forecast(model, X_start_enc, X_start_dec, n_future)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(np.arange(len(y_val[-n_past:])), y_val[-n_past:], label='True (Past)', color='blue')\n",
    "    plt.plot(np.arange(len(y_val[-n_past:])), y_pred_past, label=f'Predicted (Past) - {label}', color='orange')\n",
    "    plt.plot(np.arange(len(y_val[-n_past:]), len(y_val[-n_past:]) + n_future), future_pred, label=f'Forecast (Future) - {label}', color='green')\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'{label} - Past Predictions & Future Forecast')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for Transformer\n",
    "plot_predictions_and_forecast(model, X_val, Y_val, n_past=200, n_future=100, label=\"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 20. Plot Predictions + Long Future Forecasts\n",
    "# ===============================\n",
    "plot_predictions_and_forecast(model, X_val, Y_val, n_past=1000, n_future=400, label=\"Transformer\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
